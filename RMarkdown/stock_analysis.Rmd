---
output: 
  pdf_document:
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: svm-latex-ms.tex
    extra_dependencies: "subfig"
title: "Application of Machine Learning on Fundamental Stock Price Analysis"
author:
- name: Albina Cako, BSc
  affiliation: York University, Certificate in Machine Learning
- name: Colin Green, BSc
  affiliation: York University, Certificate in Machine Learning
- name: Lucy Zhang, BSc
  affiliation: York University, Certificate in Machine Learning
- name: Sean X. Zhang, MSc
  affiliation: York University, Certificate in Machine Learning
abstract: "Abstract:"
keywords: "stock price, fundamental analysis, machine learning, R"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
#bibliography: References_house_prices.bib
csl: cell-numeric.csl
header-includes:
    - \usepackage{hyperref}
    - \usepackage{graphicx}
---

# Introduction

## Background

The stock market is a marketplace where investors can purchase or sell shares of publicly traded companies. As of 2019, the amount of money invested in the global stock market has surpassed over $85 trillion. Since the inception of the stock market, investors have continuously sought to develop methods of improving their returns. Currently, there are two main schools of thought when it comes to stock market analysis: technical analysis and fundamental analysis.

*Technical analysis* looks at buying and selling trends of a particular stock. The core theory of technical analysis assumes that all information is already factored into the stock price. As such, technical analysis prioritizes identifying patterns or trends in time-series data to predict stock price at a particular time point.

*Fundamental analysis* attempts to measure the intrinsic value of a company by studying information from that company’s balance sheet, such as revenue or debt. Fundamental analysis attempts to identify companies that appear to be ‘undervalued’ or ‘overvalued’ to inform buy or sell recommendations.

Previous machine learning models that simulated stock market returns have largely focused on using time series data to predict stock trends, which is more akin to technical analysis. However, such models have run into challenges such as overfitting or a lack of interpretability. One benefit of fundamental analysis is that it allows the investor to learn about which aspects of a company’s financials will influence that company’s stock price; it is more interpretable. As there are dozens to hundreds of variables on a company’s balance sheet, the use of machine-learning approaches may augment fundamental analysis by pinpointing important markers of a company’s financials and their relationship with the stock price.

## Objective

In this project, we apply machine learning and data science techniques to predict the market capitalization, which is how much company is worth on the stock market. Stock price can then be calculated by dividing market capitalization by total number of stocks issued. We also create an application using R shiny to be used as a guide for investors. This application would be used individuals interested in checking their stock analyses with a machine learning prediction. The application could be used by financial analysts, portfolio managers, or non-professional investors with an interest in fundamental analysis.

# Methodology

```{r libraries, message = FALSE, warning = FALSE, echo = FALSE}

library(knitr)
library(dplyr)
library(readr)
library(finalfit)
library(cluster)
library(factoextra)
library(dendextend)
library(ggplot2)
library(FactoMineR)
library(NbClust)
library(Hmisc)
library(ggcorrplot)
library(tidyverse)
library(car)
library(caret)
library(VIF)
library(mice)
library(rpart)
library(factoextra)
```

## Data Preprocessing
The original dataset was obtained from Kaggle. Five datasets were combined together containing stock information for different years: 2014, 2015, 2016, 2017 and 2018, respectively. There were 200 columns in the dataset, however, after analyzing the data, only 66 columns were chosen as fundamental columns and were included in the project. 

## Missingness
The dataset was assessed for missing values. Any columns that had more then 1/3 of the data as missing values were removed. For the rest of the columns, data imputation was performed using the MICE package in R. We used the CART method to impute the data. CART imputes values by using classification and regression trees. Four columns were left with missing values after imputation. Those columns were removed leaving a dataset with a total of 62 columns. 

## Feature Selection
It is important to note that this project contains both unsupervised and supervised learning. Decision Tree was used for feature selection. Feature tree is a classification algorithm used for classification problems, as well as detecting variable importance in a dataset. The top 10 important variables from the decision tree were selected as the features. They were used to run k-means unsupervised learning, which determined 4 clusters of data. Then, supervised learning dataset was selected as the top 10 variables selected from the decision tree plus the cluster # (as a categorical variable) and the Sector of the stock. Thus, the unsupervised learning data contained 10 features, while the supervised learning data contained 12. 

## Principle Component Analysis
We applied Principle Component Analysis (PCA) to our feature dataset for dimension reduction before doing unsupervised learning using the k-Means clustering algorithm. PCA creates orthogonal 'principle components' of the feature set, reducing multicollinearity within the data. Since the k-means algorithm is non-parametric, reducing multicollinearity before performing k-Means clustering, could lead to greater discrimination between the clusters.

## Unsupervised Learning
The k-Means algorithm was performed in order to cluster the data before supervised learning. The number of clusters was evaluated by plotting the within-cluster sum of squares (WSS) against the number of clusters (k). The optimal number of clusters was chosen based on a combination of the 'elbow method' and domain knowledge.  

## Supervised Learning
Supervised learning was performed using three algorithms: XGBoost, Random Forest, Lasso Model and GBM Model. XGBoost is a very powerful algorithm which drives fast learning and offers efficient usage of storage. XGBoost uses ensemble learning, which is a systematic solution that combines the predictive power of multiple learners. It outputs a single model that gives the combined output from many models. This allows the opportunity to not rely on the results of a single machine learning model.In this particular model, the trees are built sequentially, such that the next tree focuses on reducing  the errors of the previous tree. Random forest is another supervised learning model that uses "ensemble" method to fit many decision trees by using a subset of the rows and then taking the "mode" of the predicted class. GBM, which stands for Gradient Boosting Machine, is also a gradient boosting algorithm that works similar to XGBoost. However, XGBoost has more tuning parameters, thus both algorithms were chosen for comparison. All models were ran and they were evaluated using the k-fold cross validation method. Three accuracy metrics: Root Mean-Squared Error (RMSE), Pearson correlation ($R^2$), and Mean Average Error (MAE) were used to chose the final model.

## Deployment

# Results

The original dataset contained 

###Data Preparation

```{r}
#load in the first file
data_2014 <- read.csv('2014_Financial_Data.csv')
data_2015 <- read.csv('2015_Financial_Data.csv')
data_2016 <- read.csv('2016_Financial_Data.csv')
data_2017 <- read.csv('2017_Financial_Data.csv')
data_2018 <- read.csv('2018_Financial_Data.csv')

#add a column for year
data_2014 <- data_2014 %>% mutate(year=2014)
data_2015 <- data_2015 %>% mutate(year=2015)
data_2016 <- data_2016 %>% mutate(year=2016)
data_2017 <- data_2017 %>% mutate(year=2017)
data_2018 <- data_2018 %>% mutate(year=2018)

#fix the column name
colnames(data_2014)[224] <- 'PRICE.VARR'
colnames(data_2015)[224] <- 'PRICE.VARR'
colnames(data_2016)[224] <- 'PRICE.VARR'
colnames(data_2017)[224] <- 'PRICE.VARR'
colnames(data_2018)[224] <- 'PRICE.VARR'

complete_data <- rbind(data_2014, data_2015, data_2016, data_2017, data_2018)

#only include fundamental columns
complete_data <- subset(complete_data[,c(1:4,6:8,10,12:14,16,20,22,30,33,34,36,38,40:43,45:53,55,56,60:74,142,176,179:190,223,226)])
complete_data <- complete_data[complete_data$X != 'IGLD', ]
complete_data <- complete_data[complete_data$X != 'SBT', ]
complete_data <- complete_data[complete_data$X != 'KST', ]
complete_data <- complete_data[complete_data$X != 'AMX', ]

```

## Missing Values
After we finished the first step of data cleaning, we want to do the data validation. For missing values, as the plot shown, a lot of observations make up the majority of the missing data and we decided to remove observations that have more than a third of the columns NA. After we removed those observations, we set the sector and year columns as a factor and saved the new data set into a new CSV files for further data exploration.
```{r}
missing_plot(complete_data)
#sort((sapply(complete_data, function(x) sum(is.na(x)))), decreasing=TRUE)

complete_data_remove<-complete_data[which(rowMeans(!is.na(complete_data))>(1/3)),]
missing_plot(complete_data_remove)
#sort((sapply(complete_data_remove, function(x) sum(is.na(x)))), decreasing=TRUE)

complete_data_remove$Sector <- as.factor(complete_data_remove$Sector)
complete_data_remove$year <- as.factor(complete_data_remove$year)

#save the new data set as a csv
#write.csv(complete_data_remove,"fundamental_data.csv")
pvq <- quantile(complete_data_remove$Market.Cap, probs = c(0.01,0.99), names=FALSE, na.rm=TRUE)
plot_data <- complete_data_remove
plot_data[plot_data==0] <- NA

```

To account for missing values, we chose to use the CART (Classification and Regression Trees) method of imputation (\hyperref[sec:fig2]{Figure 2}). Blue represents the distribution of the original data, while red represents the distribution of imputed data. After imputation, 4 columns still have missing values. 

```{r CART imputation, echo=FALSE, fig.align='center', fig.height=3, fig.width=4, fig.cap = 'CART-imputed values for sqft\\label{sec:fig2}'}
cart <- readRDS('cart_imputation.rds')
complete_cart <- complete(cart, 1)
complete_cart <- subset(complete_cart[,c(1:22,24:28,31,33:65)])
complete_cart$year <- as.factor(complete_cart$year)
write.csv(complete_cart, 'full_set.csv')
summary(complete_cart)
#still 4 columns with missing values
```

We decided to remove the 4 columns with missing values after imputation, which left a total of 62 columns for our modeling.

####Correlation Plot
There are 62 columns after we finished data cleaning, and we want to select the important features to do modeling.
We performed a correlation analysis based on Pearson's coefficient between each numeric predictor first. We considered a correlation > 0.5, with p < 0.05 as a significant correlation. \hyperref[sec:fig3]{Figure 3} demonstrates significant correlation between many of our predictor variables. 

```{r corrplot, echo=FALSE, fig.align='center', fig.height=5, fig.width=5, fig.cap='Correlogram\\label{sec:fig3}'}
#corrplot for numerical
df_full <- read.csv('full_set.csv')
df_full_numeric <- subset(df_full, select =c(3:60))
cor <- rcorr(as.matrix(df_full_numeric))
p.mat <- cor_pmat(as.matrix(df_full_numeric))
par(mfrow=c(1,1))
ggcorrplot(cor$r, type = 'upper', p.mat = p.mat, sig.level = 0.05, lab = TRUE)

```

###Data Normal Distribution
```{r data normal distribution }
plot_index <- list()
for (i in c(1:58)){
  
  plot_index[[names(df_full_numeric[i])]] <- ggplot(df_full_numeric, aes(x = df_full_numeric[[i]])) +
    stat_function(
      fun = dnorm,
      args = with(df_full_numeric, c(mean = mean(df_full_numeric[[i]], na.rm=TRUE), 
                            sd = sd(df_full_numeric[[i]], na.rm=TRUE))))+
    labs(title=as.list(names(df_full_numeric[i])), x='',y='Price Change')
  #print(plot_index[[names(df[i])]])
}

```

We also did some data visualization for our final data set which we will use for modeling.
```{r text summary, echo=FALSE}
df_full <- subset(df_full[,c(2,16,43,35,10,31,36,8,6,28,27,47)])
write.csv(df_full,'important.csv')
```

Correlation plot for the final dataset 
```{r corrplot 2, echo=FALSE, fig.align='center', fig.height=5, fig.width=5, fig.cap='Correlogram\\label{sec:fig3}'}
#corrplot for numerical
df <- read.csv('important.csv')
df_numeric <- subset(df, select =c(3:13))
cor <- rcorr(as.matrix(df_numeric))
p.mat <- cor_pmat(as.matrix(df_numeric))
par(mfrow=c(1,1))
ggcorrplot(cor$r, type = 'upper', p.mat = p.mat, sig.level = 0.05, lab = TRUE)

```

###Feature Selection

In order to do our feature selection, we ran a decision tree model to do a variable importance analysis. 
```{r variable importancy}
#decision_tree_model <-readRDS('decision_tree_model.rds')
#print(decision_tree_model)
#dTreeImp <- varImp(decision_tree_model, scale = FALSE)
#plot(dTreeImp, top = 10)
#invisible(model_importance <- summary(decision_tree_model$finalModel))


## Principle Component Analysis
We performed PCA to reduce the dimensionality of our feature dataset. The Scree plot shows the overall variance explained by each principle component. The top 5 dimensions explained approximately 90% of the total variance within the data. Individual datapoints involving large technology companies (Google, Apple, Amazon) had high contributions to the overall variance. R&D Expenses and Stock-based compensation were two variables with high contribution to variance, while Income Tax Expense and Operating Cash Flow had more negligible contribution.  

```{r scree, message = FALSE, warning = FALSE, echo = FALSE, fig.cap= 'Scree plot', fig.height=3, fig.width=3,fig.align='center'}
df_PCA <- read.csv('important.csv')
df_imputed <- read.csv('full_set.csv')
df_PCA$year <- df_imputed$year
df_PCA$uniqueticker <- paste(df_PCA$X, df_PCA$year)
rownames(df_PCA) <- df_PCA$uniqueticker
df_PCA <- df_PCA[, !names(df_PCA) %in% c('X','X.1', 'Market.Cap', 'year','uniqueticker','Sector')]
res.pca <- PCA(df_PCA, graph = FALSE)

#visualize percentage of explained variance from each dimension. first 5 explain ~85%
fviz_eig(res.pca)
```

```{r PCAind, message = FALSE, warning = FALSE, echo = FALSE, fig.cap= 'Effect of Individual points - PCA',fig.height=4,fig.width=4,fig.align='center'}
#color visualization for individual companies
fviz_pca_ind(res.pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)
```
```{r PCAvar, message = FALSE, warning = FALSE, echo = FALSE, fig.cap= 'Effect of Variables - PCA',fig.height=4,fig.width=4,fig.align='center'}
#color visualization for variables
fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             # Avoid text overlapping
)
```

## K Means Clustering

The 'elbow method' was first performed to determine an optimal number of k clusters. However, there was no significant drop in within-cluster sum of squares with k besides k=2. As two clusters did not provide much discrimination for our observations, we instead used k=4 as the final number of clusters. 
```{r elbow,fig.cap='Elbow method', out.height='50%', out.width = '70%', message = FALSE, warning = FALSE, echo = FALSE, fig.height = 4,fig.width=4}
include_graphics('unsupervised_elbow.png')
```
The following figure displays our datapoints in a 2-D space based on 4 clusters. (will show the cluster plots and more by tomorrow evening)
```{r cluster,fig.cap='K means clustering, k = 4', out.height='70%', out.width = '100%', message = FALSE, warning = FALSE, echo = FALSE}
include_graphics('cluster_image.png')
```

###Feature Selection

In order to do our feature selection, we ran a decision tree model to do a variable importance analysis. 
```{r variable importancy}
#decision_tree_model <-readRDS('decision_tree_model.rds')
#print(decision_tree_model)
#dTreeImp <- varImp(decision_tree_model, scale = FALSE)
#plot(dTreeImp, top = 10)
#invisible(model_importance <- summary(decision_tree_model$finalModel))



## Modeling
The k-fold cross-validation method evaluates the model performance on different subsets of the training data calculates the average prediction error rate. We used k = 10 for our project,and this method was used instead of the simple train-test-split as it gives a more valid estimation of model effectiveness.

###Random Forest
```{r}
Lasso_Regression_Model <- readRDS("Lasso_Model.rds")
invisible(model_importance <- summary(Lasso_Regression_Model$finalModel))
print(Lasso_Regression_Model)
```

###XGBoost
```{r}
XGB_model_albina_updated <- readRDS("XGB_model_albina_updated.rds")
invisible(model_importance <- summary(XGB_model_albina_updated$finalModel))
print(XGB_model_albina_updated)
```

###Lasso Regression
For the lasso regression model, RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.9.
```{r}
Lasso_Regression_Model <- readRDS("Lasso_Model.rds")
invisible(model_importance <- summary(Lasso_Regression_Model$finalModel))
print(Lasso_Regression_Model)
```

###GBM
The gradient boosting model was tuned by several different parameters. The final values used for the model were n.trees = 600, interaction.depth = 9, shrinkage = 0.1 and n.minobsinnode = 20
```{r}
Gradient_Boosting_model <- readRDS("GBM_Model.rds")
invisible(model_importance <- summary(Gradient_Boosting_model$finalModel))
print(Gradient_Boosting_model)
```

###Model Selection
All models found $nmnmb$ and $hghh$ to be important predictors of Market.Cap. Mean Absolute Error (MAE) tells the average error of the variable we want to predict. Root Mean-Squared Error (RMSE) is similar with MAE but it is more useful when we are interested in fewer larger errors over many small errors. Overall, we prioritize model stability and thus prioritized RMSE over MAE. $R^2$ computes how much better the regression fits the data than the mean line, which gives an overall score.For predicting market cap, we desired a model with the lowest RMSE and MAE to keep the high accuracy of prediction. The XGBoost model had the highest $R^2$ as well as the lowest RMSE and MAE, thus, it was chosen for deployment.
```{r create dataframe of model performance, echo=FALSE}
models <- c("random_forest","extreme_gradient_boosting","Lasso_Regression","gradient_boosting" )
model_performance <- data.frame(matrix(unlist(models), nrow=4, byrow=TRUE), stringsAsFactors = FALSE)
colnames(model_performance) <- c("model")
RMSE<- c(274957.8,233734.1,257316.5,220850.4)
R2 <- c(0.8067,0.846707,0.8282601,0.861169)
MAE <- c(135701,119745.2,134117.1,116308.5)
model_performance <- model_performance %>% mutate(RMSE = round(RMSE,2), R2=round(R2,2), MAE=round(MAE,2))
kable(model_performance, format = 'pipe', caption = 'Model Accuracy')
```


# Discussion