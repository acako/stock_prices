---
output: 
  pdf_document:
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: svm-latex-ms.tex
    extra_dependencies: "subfig"
title: "Application of Machine Learning on Fundamental Stock Price Analysis"
author:
- name: Albina Cako, BSc
  affiliation: York University, Certificate in Machine Learning
- name: Colin Green, BSc
  affiliation: York University, Certificate in Machine Learning
- name: Lucy Zhang, BSc
  affiliation: York University, Certificate in Machine Learning
- name: Sean X. Zhang, MSc
  affiliation: York University, Certificate in Machine Learning
abstract: "Abstract:"
keywords: "stock price, fundamental analysis, machine learning, R"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
#bibliography: References_house_prices.bib
csl: cell-numeric.csl
header-includes:
    - \usepackage{hyperref}
    - \usepackage{graphicx}
---

```{r libraries, message = FALSE, warning = FALSE, echo = FALSE}
library(knitr)
library(dplyr)
library(cluster)
library(factoextra)
library(dendextend)
library(ggplot2)
library(FactoMineR)
library(NbClust)
```

# Introduction



# Methodology

## Data Preprocessing

## Missing Values

## Outliers

## Data Curation

## Modeling

## Deployment

# Results
## Data Exploration

## Principle Component Analysis
```{r scree, message = FALSE, warning = FALSE, echo = FALSE, fig.cap= 'Scree plot'}
df_PCA <- read.csv('important.csv')
df_imputed <- read.csv('full_set.csv')
df_PCA$year <- df_imputed$year
df_PCA$uniqueticker <- paste(df_PCA$X, df_PCA$year)
rownames(df_PCA) <- df_PCA$uniqueticker
df_PCA <- df_PCA[, !names(df_PCA) %in% c('X','X.1', 'Market.Cap', 'year','uniqueticker','Sector')]
res.pca <- PCA(df_PCA, graph = FALSE)

#visualize percentage of explained variance from each dimension. first 5 explain ~85%
fviz_eig(res.pca)
```
```{r PCAind, message = FALSE, warning = FALSE, echo = FALSE, fig.cap= 'Individual points - PCA'}
#color visualization for individual companies
fviz_pca_ind(res.pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)
```
```{r PCAvar, message = FALSE, warning = FALSE, echo = FALSE, fig.cap= 'Variables - PCA'}
#color visualization for variables
fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             # Avoid text overlapping
)
```

## K Means Clustering
```{r elbow,fig.cap='Elbow method', out.height='70%', out.width = '100%', message = FALSE, warning = FALSE, echo = FALSE}
include_graphics('unsupervised_elbow.png')
```

```{r cluster,fig.cap='K means clustering, k = 4', out.height='70%', out.width = '100%', message = FALSE, warning = FALSE, echo = FALSE}
include_graphics('cluster_image.png')
```

## Modeling
###Random Forest
###XGBoost
###Lasso Regression
###GBM
###Model Selection

# Discussion