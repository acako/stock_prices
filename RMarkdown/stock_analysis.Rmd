---
output: 
  pdf_document:
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: svm-latex-ms.tex
    extra_dependencies: "subfig"
title: "Application of Machine Learning on Fundamental Stock Price Analysis"
author:
- name: Albina Cako, BSc
  affiliation: York University, Certificate in Machine Learning
- name: Colin Green, BSc
  affiliation: York University, Certificate in Machine Learning
- name: Lucy Zhang, BSc
  affiliation: York University, Certificate in Machine Learning
- name: Sean X. Zhang, MSc
  affiliation: York University, Certificate in Machine Learning
abstract: "Abstract:"
keywords: "stock price, fundamental analysis, machine learning, R"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
#bibliography: References_house_prices.bib
csl: cell-numeric.csl
header-includes:
    - \usepackage{hyperref}
    - \usepackage{graphicx}
---

```{r libraries, message = FALSE, warning = FALSE, echo = FALSE}
library(knitr)
library(dplyr)
library(readr)
library(finalfit)
library(cluster)
library(factoextra)
library(dendextend)
library(ggplot2)
library(FactoMineR)
library(NbClust)
library(Hmisc)
library(ggcorrplot)
library(tidyverse)
library(car)
library(caret)
library(VIF)
library(mice)
library(rpart)
library(factoextra)
```




# Introduction

## Background

The stock market is a marketplace where investors can purchase or sell shares of publicly traded companies. As of 2019, the amount of money invested in the global stock market has surpassed over $85 trillion. Since the inception of the stock market, investors have continuously sought to develop methods of improving their returns. Currently, there are two main schools of thought when it comes to stock market analysis: technical analysis and fundamental analysis.

*Technical analysis* looks at buying and selling trends of a particular stock. The core theory of technical analysis assumes that all information is already factored into the stock price. As such, technical analysis prioritizes identifying patterns or trends in time-series data to predict stock price at a particular time point.

*Fundamental analysis* attempts to measure the intrinsic value of a company by studying information from that company’s balance sheet, such as revenue or debt. Fundamental analysis attempts to identify companies that appear to be ‘undervalued’ or ‘overvalued’ to inform buy or sell recommendations.

Previous machine learning models that simulated stock market returns have largely focused on using time series data to predict stock trends, which is more akin to technical analysis. However, such models have run into challenges such as overfitting or a lack of interpretability. One benefit of fundamental analysis is that it allows the investor to learn about which aspects of a company’s financials will influence that company’s stock price; it is more interpretable. As there are dozens to hundreds of variables on a company’s balance sheet, the use of machine-learning approaches may augment fundamental analysis by pinpointing important markers of a company’s financials and their relationship with the stock price.

## Objective

In this project, we apply machine learning and data science techniques to predict the market capitalization, which is how much company is worth on the stock market. Stock price can then be calculated by dividing market capitalization by total number of stocks issued. We also create an application using R shiny to be used as a guide for investors. This application would be used individuals interested in checking their stock analyses with a machine learning prediction. The application could be used by financial analysts, portfolio managers, or non-professional investors with an interest in fundamental analysis.


# Methodology

## Data Preprocessing

## Missing Values

## Data Curation

## Modeling

## Deployment

# Results
## Data Exploration

###Data Preparetion
The original data is from Kaggle and have several different CSV files include the stock information for different years. We combined the CSV files into one full data set for our project
```{r}
#load in the first file
data_2014 <- read.csv('2014_Financial_Data.csv')
data_2015 <- read.csv('2015_Financial_Data.csv')
data_2016 <- read.csv('2016_Financial_Data.csv')
data_2017 <- read.csv('2017_Financial_Data.csv')
data_2018 <- read.csv('2018_Financial_Data.csv')

#add a column for year
data_2014 <- data_2014 %>% mutate(year=2014)
data_2015 <- data_2015 %>% mutate(year=2015)
data_2016 <- data_2016 %>% mutate(year=2016)
data_2017 <- data_2017 %>% mutate(year=2017)
data_2018 <- data_2018 %>% mutate(year=2018)

#fix the column name
colnames(data_2014)[224] <- 'PRICE.VARR'
colnames(data_2015)[224] <- 'PRICE.VARR'
colnames(data_2016)[224] <- 'PRICE.VARR'
colnames(data_2017)[224] <- 'PRICE.VARR'
colnames(data_2018)[224] <- 'PRICE.VARR'

complete_data <- rbind(data_2014, data_2015, data_2016, data_2017, data_2018)

#only include fundamental columns
complete_data <- subset(complete_data[,c(1:4,6:8,10,12:14,16,20,22,30,33,34,36,38,40:43,45:53,55,56,60:74,142,176,179:190,223,226)])
complete_data <- complete_data[complete_data$X != 'IGLD', ]
complete_data <- complete_data[complete_data$X != 'SBT', ]
complete_data <- complete_data[complete_data$X != 'KST', ]
complete_data <- complete_data[complete_data$X != 'AMX', ]

```

After we finished the first step of data cleaning, we want to do the data validation. For missing values, as the plot shown, a lot of observations make up the majority of the missing data and we decided to remove observations that have more than a third of the columns NA.After we removed those observations, we set the sector and year columns as a factor and saved the new data set into a new CSV files for futhur data exploration.
```{r}
missing_plot(complete_data)
#sort((sapply(complete_data, function(x) sum(is.na(x)))), decreasing=TRUE)

complete_data_remove<-complete_data[which(rowMeans(!is.na(complete_data))>(1/3)),]
missing_plot(complete_data_remove)
#sort((sapply(complete_data_remove, function(x) sum(is.na(x)))), decreasing=TRUE)

complete_data_remove$Sector <- as.factor(complete_data_remove$Sector)
complete_data_remove$year <- as.factor(complete_data_remove$year)

#save the new data set as a csv
#write.csv(complete_data_remove,"fundamental_data.csv")
pvq <- quantile(complete_data_remove$Market.Cap, probs = c(0.01,0.99), names=FALSE, na.rm=TRUE)
plot_data <- complete_data_remove
plot_data[plot_data==0] <- NA

```

To account for missing values, we chose to use the CART (Classification and Regression Trees) method of imputation (\hyperref[sec:fig2]{Figure 2}). Blue represents the distribution of the original data, while red represents the distribution of imputed data. After the imputation there are still 4 columns has missing values.

```{r CART imputation, echo=FALSE, fig.align='center', fig.height=3, fig.width=4, fig.cap = 'CART-imputed values for sqft\\label{sec:fig2}'}
cart <- readRDS('cart_imputation.rds')
complete_cart <- complete(cart, 1)
complete_cart <- subset(complete_cart[,c(1:22,24:28,31,33:65)])
complete_cart$year <- as.factor(complete_cart$year)
write.csv(complete_cart, 'full_set.csv')
summary(complete_cart)
#still 4 columns with missing values
```

###Feature Selection

####Correlation Plot
There are 62 columns after we finished data cleaning, and we want to select the important features to do modeling.
We performed a correlation analysis based on Pearson's coefficient between each numeric predictor first. We considered a correlation > 0.5, with p < 0.05 as a significant correlation. \hyperref[sec:fig3]{Figure 3} demonstrates significant correlation between many of our predictor variables. 

```{r corrplot, echo=FALSE, fig.align='center', fig.height=5, fig.width=5, fig.cap='Correlogram\\label{sec:fig3}'}
#corrplot for numerical
df_full <- read.csv('full_set.csv')
df_full_numeric <- subset(df_full, select =c(3:60))
cor <- rcorr(as.matrix(df_full_numeric))
p.mat <- cor_pmat(as.matrix(df_full_numeric))
par(mfrow=c(1,1))
ggcorrplot(cor$r, type = 'upper', p.mat = p.mat, sig.level = 0.05, lab = TRUE)

```

###Data Normal Distribution
```{r data normal distribution }
plot_index <- list()
for (i in c(1:58)){
  
  plot_index[[names(df_full_numeric[i])]] <- ggplot(df_full_numeric, aes(x = df_full_numeric[[i]])) +
    stat_function(
      fun = dnorm,
      args = with(df_full_numeric, c(mean = mean(df_full_numeric[[i]], na.rm=TRUE), 
                            sd = sd(df_full_numeric[[i]], na.rm=TRUE))))+
    labs(title=as.list(names(df_full_numeric[i])), x='',y='Price Change')
  #print(plot_index[[names(df[i])]])
}

```

####Varaible Importancy
We decided to use decision tree to check the variable importance as a important reference for us to do feature selection.
```{r variable importancy}
#decision_tree_model <-readRDS('decision_tree_model.rds')
#print(decision_tree_model)
#dTreeImp <- varImp(decision_tree_model, scale = FALSE)
#plot(dTreeImp, top = 10)
#invisible(model_importance <- summary(decision_tree_model$finalModel))

```


We also did some data visualization for our final data set which we will use for modeling.
```{r text summary, echo=FALSE}
df_full <- subset(df_full[,c(2,16,43,35,10,31,36,8,6,28,27,47)])
write.csv(df_full,'important.csv')
```

Correlation plot for the final dataset 
```{r corrplot 2, echo=FALSE, fig.align='center', fig.height=5, fig.width=5, fig.cap='Correlogram\\label{sec:fig3}'}
#corrplot for numerical
df <- read.csv('important.csv')
df_numeric <- subset(df, select =c(3:13))
cor <- rcorr(as.matrix(df_numeric))
p.mat <- cor_pmat(as.matrix(df_numeric))
par(mfrow=c(1,1))
ggcorrplot(cor$r, type = 'upper', p.mat = p.mat, sig.level = 0.05, lab = TRUE)

```


## Principle Component Analysis
```{r scree, message = FALSE, warning = FALSE, echo = FALSE, fig.cap= 'Scree plot', fig.height=3, fig.width=3,fig.align='center'}
df_PCA <- read.csv('important.csv')
df_imputed <- read.csv('full_set.csv')
df_PCA$year <- df_imputed$year
df_PCA$uniqueticker <- paste(df_PCA$X, df_PCA$year)
rownames(df_PCA) <- df_PCA$uniqueticker
df_PCA <- df_PCA[, !names(df_PCA) %in% c('X','X.1', 'Market.Cap', 'year','uniqueticker','Sector')]
res.pca <- PCA(df_PCA, graph = FALSE)

#visualize percentage of explained variance from each dimension. first 5 explain ~85%
fviz_eig(res.pca)
```
```{r PCAind, message = FALSE, warning = FALSE, echo = FALSE, fig.cap= 'Effect of Individual points - PCA',fig.height=4,fig.width=4,fig.align='center'}
#color visualization for individual companies
fviz_pca_ind(res.pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)
```
```{r PCAvar, message = FALSE, warning = FALSE, echo = FALSE, fig.cap= 'Effect of Variables - PCA',fig.height=4,fig.width=4,fig.align='center'}
#color visualization for variables
fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             # Avoid text overlapping
)
```

## K Means Clustering
```{r elbow,fig.cap='Elbow method', out.height='70%', out.width = '100%', message = FALSE, warning = FALSE, echo = FALSE}
include_graphics('unsupervised_elbow.png')
```

```{r cluster,fig.cap='K means clustering, k = 4', out.height='70%', out.width = '100%', message = FALSE, warning = FALSE, echo = FALSE}
include_graphics('cluster_image.png')
```

## Modeling
The k-fold cross-validation method evaluates the model performance on different subsets of the training data calculates the average prediction error rate. We used k = 10 for our project,and this method was used instead of the simple train-test-split as it gives a more valid estimation of model effectiveness.

###Random Forest
```{r}
Lasso_Regression_Model <- readRDS("Lasso_Model.rds")
invisible(model_importance <- summary(Lasso_Regression_Model$finalModel))
print(Lasso_Regression_Model)
```

###XGBoost
```{r}
XGB_model_albina_updated <- readRDS("XGB_model_albina_updated.rds")
invisible(model_importance <- summary(XGB_model_albina_updated$finalModel))
print(XGB_model_albina_updated)
```

###Lasso Regression
For the lasso regression model, RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.9.
```{r}
Lasso_Regression_Model <- readRDS("Lasso_Model.rds")
invisible(model_importance <- summary(Lasso_Regression_Model$finalModel))
print(Lasso_Regression_Model)
```

###GBM
The gradient boosting model was tuned by several different parameters. The final values used for the model were n.trees = 600, interaction.depth = 9, shrinkage = 0.1 and n.minobsinnode = 20
```{r}
Gradient_Boosting_model <- readRDS("GBM_Model.rds")
invisible(model_importance <- summary(Gradient_Boosting_model$finalModel))
print(Gradient_Boosting_model)
```

###Model Selection
All models found $nmnmb$ and $hghh$ to be important predictors of Market.Cap. Mean Absolute Error (MAE) tells the average error of the variable we want to predict. Root Mean-Squared Error (RMSE) is similar with MAE but it is more useful when we are interested in fewer larger errors over many small errors. Overall, we prioritize model stability and thus prioritized RMSE over MAE. $R^2$ computes how much better the regression fits the data than the mean line, which gives an overall score.For predicting market cap, we desired a model with the lowest RMSE and MAE to keep the high accuracy of prediction. The XGBoost model had the highest $R^2$ as well as the lowest RMSE and MAE, thus, it was chosen for deployment.
```{r create dataframe of model performance, echo=FALSE}
models <- c("random_forest","extreme_gradient_boosting","Lasso_Regression","gradient_boosting" )
model_performance <- data.frame(matrix(unlist(models), nrow=4, byrow=TRUE), stringsAsFactors = FALSE)
colnames(model_performance) <- c("model")
RMSE<- c(274957.8,233734.1,257316.5,220850.4)
R2 <- c(0.8067,0.846707,0.8282601,0.861169)
MAE <- c(135701,119745.2,134117.1,116308.5)
model_performance <- model_performance %>% mutate(RMSE = round(RMSE,2), R2=round(R2,2), MAE=round(MAE,2))
kable(model_performance, format = 'pipe', caption = 'Model Accuracy')
```


# Discussion